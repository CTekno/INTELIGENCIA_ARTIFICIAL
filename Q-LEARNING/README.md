# Q-Learning com OpenAI Gym: Taxi-v3 ğŸš•

Este notebook implementa um sistema de busca no modelo A*, programado em Python. O objetivo do algoritmo Ã© encontrar o caminho mais vantajoso atÃ© a cidade de **Curitiba**, partindo da cidade **Porto UniÃ£o**. Pense em um sistema de navegaÃ§Ã£o ğŸš—.

## DescriÃ§Ã£o ğŸ”

Este projeto implementa um algoritmo de aprendizado por reforÃ§o chamado Q-learning para treinar um agente a resolver o ambiente Taxi-v3 da biblioteca OpenAI Gym. O objetivo Ã© ensinar o agente tÃ¡xi a pegar um passageiro em um local e deixÃ¡-lo em outro no caminho mais curto possÃ­vel, evitando penalidades por movimentos ilegais.

## InstalaÃ§Ã£o âš’ï¸

**DependÃªncias**\
Para executar este projeto, vocÃª precisa das seguintes bibliotecas Python:

* numpy
* gym==0.17.3
* random
* time
* IPython
#
```python
!pip install gym==0.17.3
```

## CÃ³digo ğŸ”¢
### 1. ConfiguraÃ§Ã£o do Ambiente:

```python
import gym
env = gym.make('Taxi-v3')
```
### 2. InicializaÃ§Ã£o da Tabela Q:
```python
q_table = np.zeros([env.observation_space.n, env.action_space.n])
```


### 3. Treinamento do Agente: ğŸ‹ï¸

O agente Ã© treinado em 100.000 episÃ³dios.
Os parÃ¢metros do Q-learning usados sÃ£o:

* alpha = 0.1    **(Taxa de aprendizado)**
* gamma = 0.6    **(Fator de desconto)**
* epsilon = 0.1 **(Taxa de exploraÃ§Ã£o)**

#### Loop de treinamento:
```python
for i in range(100000):
    estado = env.reset()
    penalidades, recompensa = 0, 0
    done = False

    while not done:
        if random.uniform(0, 1) < epsilon:
            acao = env.action_space.sample()
        else:
            acao = np.argmax(q_table[estado])

        proximo_estado, recompensa, done, info = env.step(acao)

        q_antigo = q_table[estado, acao]
        proximo_maximo = np.max(q_table[proximo_estado])

        q_novo = (1 - alpha) * q_antigo + alpha * (recompensa + gamma * proximo_maximo)
        q_table[estado, acao] = q_novo

        if recompensa == -10:
            penalidades += 1

        estado = proximo_estado

    if i % 100 == 0:
        clear_output(wait=True)
        print('EpisÃ³dio: ', i)

```

### 4. AvaliaÃ§Ã£o:

O agente treinado Ã© avaliado em 50 episÃ³dios para verificar seu desempenho.

```python
total_penalidades = 0
episodios = 50
frames = []

for _ in range(episodios):
    estado = env.reset()
    penalidades, recompensa = 0, 0
    done = False
    while not done:
        acao = np.argmax(q_table[estado])
        estado, recompensa, done, info = env.step(acao)

        if recompensa == -10:
            penalidades +=1

        frames.append({
           'frame': env.render(mode='ansi'),
           'state': estado,
           'action': acao,
           'reward': recompensa
           })

    total_penalidades += penalidades

print('EpisÃ³dio, ', episodios)
print('Penalidades, ', total_penalidades)

```
### 5. VisualizaÃ§Ã£o:
O desempenho do agente pode ser visualizado usando os frames armazenados.
```python
for frame in frames:
    clear_output(wait=True)
    print(frame['frame'])
    print('Estado', frame['state'])
    print('AÃ§Ã£o', frame['action'])
    print('Recompensa', frame['reward'])
    sleep(.7)

```

## ConsideraÃ§Ãµes finais: ğŸ¤“â˜ï¸
### **Notas**
A versÃ£o do gym usada neste projeto nÃ£o requer renderizaÃ§Ã£o grÃ¡fica, tornando-o adequado para execuÃ§Ã£o em linha de comando.

## ConclusÃ£o:

Este projeto demonstra a aplicaÃ§Ã£o do Q-learning, um algoritmo de aprendizado por reforÃ§o sem modelo, para resolver o ambiente Taxi-v3. Ao final do treinamento, o agente tÃ¡xi aprende a navegar pelo ambiente de forma eficiente, minimizando penalidades e completando as tarefas de maneira otimizada.

Sinta-se Ã  vontade para experimentar o cÃ³digo e ajustar os parÃ¢metros para ver como o desempenho do agente se altera.
